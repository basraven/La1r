'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/conceptual-setup/','title':"Conceptual Setup",'content':"Conceptual details To give a 1-minute overview: this is the conceptual model driving La1r: It shows the conceptual layout of how all components in the Event-Driven architecture are structured. Since we are now only focussing on the conceptual aspects, implementation details such as infrastructure stack are not discussed here. The fundamental architecture for La1r follows a Kappa architectural pattern for processing data. This means that the architecture will handle bulk/batch data the same way as it handles streaming/realtime data.\nThere has been made a distinction between two types of data streams:\n Raw Data Stream - Data that is not \u0026ldquo;Governed\u0026rdquo; and does not apply to the imposed event structures as described on this site. This is often the data sink for commercial-of-the-shelf (COTS) components with which need to be integrated. To save the hassel of writing custom extensions to those components, it is easier to push the data to a \u0026ldquo;raw\u0026rdquo; even stream and transform the raw events into structured events which conform to the even structures. Structured Data Stream - This data stream only contains data which conforms to the defined standard for events  To summarize, we identify several conceptual components:\n Raw Data producer - Any sensor, smart camera, etc. which is hooked up to the raw data stream and produces data from which well formed events can be produced. Raw Data Stream - The data vehicle which stores all incoming data which can be used to create events off, this can be very raw measurement data of data structured in an application specific format Streaming Event Transformations - This can be any application which is connected to the Raw Data Stream and is able to create events conforming to the Event Specifications based on the data coming from the Raw Data Stream (this is non-restrictive and can also come from other places) Event Specifications - All specifications used to structure all events which are published on the Structured Event Stream. These Events specifications will also be published on la1r.com. These event specifications are not a direct part of the actual data flow, but are of a significant enough importance to name it in this diagram. Structured Event Stream - The data stream which stores the structured events, forming the logical epi-center of the event-driven la1r. The majority of the events are sourced by transformed raw data from the Raw Data Stream or results of analyzed raw/structured events Streaming Analytics Processes - This is identical to the Automated Event Transformation, only these processes analyze the data to find significant patterns which can be used by other (decoupled) processes futher downstream. Structured Event Consumer - This can be any device which consumes events which are published on the Structured Event Stream and acts on it with a certain behavior, for example a light switching on based on an event. This consumer also entails translating the Structured Event into a format a device is able to operate on.  Conceptual Architecture Principles The la1r architecture followes several conceptual principles which components in its architecture should follow. Since this will not capture implementation specific / techncial principles, a section on technical principles is describe in the technical setup page\n Data is realtime and streaming - Always assume that data, streaming throught the la1r infrascture is in streaming \u0026ldquo;format\u0026rdquo;. Do not unneccesarily store it, or batch it when realtime streaming solutions can also be applied Don't assume information share - since an enterprise environment is conceptually simulated, it should also be simulated that (conceptual) teams are not fully aware of all integrations made by other (conceptual) teams. The implications of this is that there is a need for decoupling and formal informations definitions. An example of this is the site you're currently reading, but further efforts should be made such as formal separation of layers, environments and data to appropriately conform to this conceptual requirement. Decentralized application paradigms where possible - To support the horizontal scaling capabilities, an effort should be made to apply decentralized paradigms, which often improve scalability and availability when implemented correctly.  Even Specifications Since there needs to be a way of formally converging to an aligned data setup, a formal event specification setup is made. This event specification will dictate how all events in the structured stream should be shaped. Events not conforming to this standard can be disregarded.\nRead more\n Governance Catalogs Since we are still \u0026ldquo;simulating\u0026rdquo; an enterprise environment, and since my own memory is sub-optimal, appropriate governance catalogs need to be setup to fully capture the IT landscape on several domains.\nRead more\n   "});index.add({'id':1,'href':'/docs/technical-setup/','title':"Technical Setup",'content':"How it's made The technical setup of La1r is my personal take on how to properly implement the conceptual setup as described on this site. This does not mean that all implementations are matching the infrastructure context, on the contrary. Implementations in La1r are aimed on enterprise scale setups, which often make them an overkill for the hardware it is running. The reason for this is that the technical setup tries to comply to several application architecture principles which are focussed on maximizing my personal learning experiences and are often near enterprise scale.\nMain area's of the technical setup The technical setup can be divided into area's:\nData Processing All the components which focus on processing data to fit in in the appropriate schema and to analyze the processed data with for example AI.\nRead more\n Ansible All bare-metal setup related Ansible scripts. This is mainly used to setup new hardware and to manage hardware such as IOT devices or new servers (improving portability).\nRead more\n Kubernetes All Kubernetes related scripts, the core of La1r! All components will be documented.\nRead more\n Secrets Small document describing how secrets are managed in the different Ansible and k8s clusters.\nRead more\n  Technical architecture principles The la1r architecture followes several techical principles which components in its architecture should follow. Since this will not capture conceptual principles, a section on conceptual principles is describe in the conceptual setup page\n Only the paranoid survive, apply and practice backup scenarios. - Backup scenarios should not only be implemented as tick in the box for our list of non functional requirements (nfrs), but should also be practiced where possible. Aim for near horizontal scaling - all services should be able to scale with cluster size. The infrastructure architecture of my current implementation is rather rigid, but the applications on it should be aimed on flexible and horizontally scalable underlying infrastructure. Behind the VPN (openvpn) by default - since this is still a learning and experimental environment, I don't want to think about security first, every step of the way. This is why the master La1r server hosts a VPN virtual network. All services and internal dns are using that entrypoint by default. This does not mean that nothing is exposed to the outside world, but only the services explicitly exposed through the online-traefik instance are  "});index.add({'id':2,'href':'/docs/technical-setup/data-processing/','title':"Data Processing",'content':"Data Processing There are several setups used in the area of Data Processing. They all focus on contribution enhanced \u0026ldquo;intelligent\u0026rdquo; decision making in the La1r setup.\n(Streaming) Transformations Since there can be a large difference between how data is received, for example for commercial of the shelf (COTS) applications, and how it should be structured conforming to the described event standard, there is a need for (streaming) Transformations.\nRead more\n (Streaming) Analytics To enhance La1r with intelligent analytics and decision making, streaming analytics is applied to facilitate these needs.\nRead more\n  "});index.add({'id':3,'href':'/docs/planning/','title':"Planning",'content':"What's next? Since I'm (currently) only developing la1r by myself, there are only so many things you can do at once. (Feel free to reach out through Github if you want to get involved!) For this reason I created this planning page in which I track and prioritize what I will add to la1r next. Feel free to add comments on this through Github!\nIn progress  Spark 2.x Cluster in k8s Streaming analyitics pipeline with Spark 2.x and Kafka  Planned  Traefik auth proxy middleware with Authelia Streaming Facial recognition from images and streaming video Object recognition (garbage bin outside of our house) combined with garbage collection ical (https://inzamelkalender.gad.nl/ical/0402200001574396) Formal managed bare-metal security camera setup \u0026ldquo;View in repo\u0026rdquo; button for all pages of the la1r documentation. While reading documentation, for example about Ansible, the visitor should be able to view which scripts are currently discussed by clicking a button to the git repository. Refactor mosquitto to vernemq  Parked  Deploy a Kafka bus as structured event bus instead of mqtt  Parked due to server resources shortage   An event data dashboard for kafka Ceph file system based rook.io or on this guide: https://owncloud.org/news/running-owncloud-in-kubernetes-with-rook-ceph-storage-step-by-step/  Completed  OpenLDAP implementation Formal service bus definition and separation of raw data and defined event data in MQTT find-lf - wifi tracking as input events on the Event Bus based on AI models  "});index.add({'id':4,'href':'/docs/conceptual-setup/event-specifications/','title':"Event Specifications",'content':"Event Specifications The event specifications are a exhaustive list of specifications to which any event should conform. With specifying this, decoupled and predictable interactions between services can be achieved.\nThese types of event specifications are currently defined:\n Content specifications - Specifications putting requirements on the content of the event Attribute specifications - Specifications putting requirements on the attributes of an event  Content specifications  All content of an event should be structured in JSON format There cannot be duplicate data between content and attribute data All events should contain a Unix epoch timestamp with Amsterdam as timezone, \u0026ldquo;unixts\u0026rdquo; should be foramted as %i All events should contain an \u0026ldquo;origin\u0026rdquo; reference (e.g. \u0026ldquo;automated/face-recognition/1\u0026rdquo;) The payload of an event can reference external data with \u0026ldquo;payload\u0026rdquo; or can contain string or blob information:  Direct string, blob or other payload formats such as a JSON object Direct source path in cephFS (spoofed until ceph is implemented) Direct protocol link (e.g. tcp://videostream1.bas) Hyperlink (e.g. https://videostream1.la1r.com)    Content Examples  Example 1, a payload with a link { \u0026#34;unixts\u0026#34; : 1578157000, \u0026#34;origin\u0026#34; : \u0026#34;manual/light-switch/6\u0026#34;, \u0026#34;payload\u0026#34; : \u0026#34;tcp://videostream1.la1r.com\u0026#34; }  Example 2, a payload with a JSON blob { \u0026#34;unixts\u0026#34; : 1577157000, \u0026#34;origin\u0026#34; : \u0026#34;automated/picture-on-login/pc-1\u0026#34;, \u0026#34;payload\u0026#34; : { \u0026#34;online-time-seconds\u0026#34; : 3212, \u0026#34;capture-location\u0026#34; : \u0026#34;/captures/temp/weekly-cleaned/picture-on-login/pc-1/1.jpeg\u0026#34; } }   Attribute specifications  The topic modelling standard should be applied on any event published Attributes need to be as specific as possible, it should not attempt to group multiple entities because this can be achieved by more advanced queries in the event bus protocol by the use of wildcards.  Topic modelling standard Topic modelling is a conceptual architectural decision which should be made very conscious of future extensions. Large refactor movements in the conceptual structure of an Event bus can have a major change impact because many applications need to change their way of interacting.\nWhat needs to be captured in topics The following things arose while brainstorming about what should be captured in a La1r structured event bus:\n This list will probably be extended in the (near) future\n  Any of the flow types  Actuator - such as lights that can dim of switch, these can be binary, stepped or by value. Sensor - such as temperature, presence, location or humidity sensors Intent - this can be behavior, derived from manual input from a person, or based on automated (AI) analytics   Location specificity - is something specific for a location (in the la1r / outside the la1r)? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not location specific. Person or device specificity - is something specific for a person or device? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not person or device specific.  Topic hierarchy Taking this into consideration, all events in the structured event bus need to following this topic hierarchy (all lower case, without spaces or special characters), either indirect by translation from the raw event bus or direct when considering these standards:\n\u0026lt;flow type\u0026gt;/\u0026lt;Location specification\u0026gt;/\u0026lt;Person or device specification\u0026gt; Examples of hierarchy usage  Example 1 - turing off light 1 in the living room actuator/living-room/lightswitch-1  Example 2 - security camera 1 sensing an unidentified person sensor/front-door/doorbel-camera-1  Example 3 - analysis algo 1 predicts an intent to shutdown all lights in the backyard intent/backyard/lightswitch-all   "});index.add({'id':5,'href':'/docs/conceptual-setup/governance-catalogs/','title':"Governance Catalogs",'content':"Governance Catalogs Since we are still \u0026ldquo;simulating\u0026rdquo; an enterprise environment, and since my own memory is sub-optimal, appropriate governance catalogs need to be setup to fully capture the IT landscape on several domains:\n Application Catalogs - These capture which applications are currently running and are in scope of the la1r environments. Data Catalgos - These capture what important data is used where (in the landscape) and again, if it is not in the data catalog it is not regarded as part of the scope of La1r.  The concept of governance catalogs ia a reoccuring pattern which large organizations often lack (or at least lack maturity to appropriately apply) in their landscape. A simple princple of \u0026ldquo;if it is not in the domain catalog, it doesn't exist\u0026rdquo; can be regarded in La1r, forcing well-practised governance.\nWider topic of governance Since governance is a topic which goes far beyond the reach of a catalog, you can regard this page as far from complete. But to ensure I use my time as efficient as possible, I will (now) not dive too deep into other governance practises.\n"});index.add({'id':6,'href':'/docs/technical-setup/ansible/','title':"Ansible",'content':"Ansible to prepare the playground Since there's always a need for installing packages on the nodes directly and I don't want to just use a bunch of shell scripts all configuration and applications outside of k8s is deployed with Ansible which are directed by makefiles. Makefiles because I don't want to remember all commands that I need to spin up ansible by heart,Ansible because I want to semi-formalize the steps I take.\nEverything on bare-metal Since I cannot put every step I take in Kubernetes, an example of this is how to setup Kubernetes itself, there is an need for a system such as Ansible. The goal here is to document every step, it does not matter how small, into an Ansible Playbook script. These Ansible scripts can be found on https://github.com/basraven/la1r/ansible\nMakefiles as operators Since I want to formalize everything into scripts, there needs to be a way to formalize how to call the different playbook with the appropriate arguments. This is why the Git repository contains 2 makefiles. There has been chosen for makefiles because the way these files are called is extremely predictable make \u0026lt;your command\u0026gt;:\n Makefile for Ansible - This makefile contains all the Ansible Playbook calls which are made to construct la1r on bare metal Makefile for Kubernetes - This makefile contains all the used Kubernetes calls to setup the Kubernetes nodes. This also contains node setup scripts suchs as applying taints.  "});index.add({'id':7,'href':'/docs/technical-setup/kubernetes/','title':"Kubernetes",'content':"How we do Kubernetes (k8s) K8s can be implemented in a million different ways. The implementation on La1r follows a few principles:\n K8s should run on bare metal only - it should not be dependent on any cloud resource, this includes function-as-service (FaaS) implementations such as AWS Lambda. Vanilla k8s should be used - flavored varients of k8s take some controls away from the sys admin and introduce (sometimes unwanted) abstractions. The latest k8s version should be used - this to incentive application of new k8s capabilities Templating engines, package managers and operators such as helm are not used - this again takes away a lot of control from the sys admin which takes away the learning experience, thus the fun. K8s over bare metal - since the use of k8s incentives portability of applications, the default hosting approach should be on k8s not on the bare metal server itself (for example through Ansible). Only things that really make sense or are directly needed by the cluster itself, such as openvpn, can be implemented directly on the server with Ansible.  Makefile as the operator The way the Kubernetes setup is done should also be properly documented to ensure portability of the setup. For this reason we document all the steps taken in the Kuberentes Makefile. This makefile should contain all the steps taken to setup and apply the yaml definitions that can be found in the Git repository\nInit with kubeadm Since we're using vanilla k8s on bare metal but we do not have all the time in the world, the decision was made to use kubeadm to initialize the cluster and manage node cluster and upgrades. Several upgrades were already performed with this which went flawless, even with multi cpu (x86 and arm7) architecture environments.\nNetwork with weaveworks Initially the decision was made to use flannel as network provider, since this is a pretty standard choice for many k8s implementations. Unfortunately this gave several networking, performance and upgrading issues over time, especially with our multi cpu architecte environment. After a tool selection process weave works came out best because:\n Substantial performance and stability improvements Capable of complex network segregations which flannel was not able to do Easy to setup, even if several sources on the way point out the opposite Bonus: Weave works supplies a fancy and comprehensive dashboard of your entire network  Node agnostic storage (coming soon) Storage is often a difficult topic with k8s bare metal clusters, since the entire aim of k8s is to be independent and decoupled from infrastructure. This is why the current plan is to migrate all storage used by the cluster to a storage abstraction service, creating node agnostic storage facilities everywhere on the k8s cluster. The currently plan is to achieve this with ceph through rook.io, which also is an exception on the non-operator application architecture principle as mentioned earlier\nReverse proxy, with dmz-ed online exposure Traefik is used as reverse proxy for all default traffic, this because it easily integrates with k8s and it provides a fancy dashboard UI, and maybe because I'm traefik 2.0 contributer\u0026hellip; Here my personal preference is to not follow the custom resource definition (CRD) hype and just use standard ingress objects, also for possible portability reasons if I ever get sick of traefik.\n"});index.add({'id':8,'href':'/docs/technical-setup/secrets/','title':"Secrets",'content':"Secrets, certificates and passwords Since sensitive data objects such as secrets, certificates and password need to be stored somewhere, and since that location is not my awful memory (I'm prone to memory leaks for some reason which are apparently still impossible to solve) I need a way to store this information. An option could be an Ansible secret vault, but since I'm not a madman who loves tempting others with putting their encrypted castle keys on git and that doesn't feel future-proof to me, \u0026ldquo;stay quantum save kids!\u0026rdquo; I took a very low level approach:\nOn each git clone on machines I need to use, I also store a /credentials folder which stores all credentials needed for my cluster. I know it's not save either, and I should for sure pgp encrypt that stuff, it still feels more safe than the carrot-stick approach\n"});index.add({'id':9,'href':'/docs/technical-setup/data-processing/streaming-analyitics/','title':"(Streaming) Analytics",'content':"Streaming analyitics as default Since La1r is applying a Kappa architecture (see conceptual setup for more details on this), it is essential that as many of it's processes occur in a streaming fasion. This also includes all the performed analytics. Streaming analytics brings new considerations, such as messaging ordering and quality of prefix data. Since these concepts are handled out-of-the-box (OOTB) in Spark 2.x, Spark 2.x is considered as the default method of applying streaming analytics.\n"});index.add({'id':10,'href':'/docs/technical-setup/data-processing/streaming-transformations/','title':"(Streaming) Transformations",'content':"Streaming transformations A serveral scenario's in la1r, a (streaming) transformation needs to be made to get the raw streaming data in the structured shape. This is often the case because there are several integrations with commercial-of-the-shelf (COTS) products which do not follow the same structured model.\nNifi and GUI as default Transformations should be easily adjustable to quickly fit the changing data needs. For this reason Nifi is used as the default tool for (streaming) data transformations. This is where Nifi, with its easily adjustable GUI can facilitate for accomodating (with realtime changes). Using a COTS solution such as Nifi also helps to reduce the amount of code that needs to be written for performing (sometimes insignificant) transformations.\n"});index.add({'id':11,'href':'/docs/','title':"Docs",'content':""});index.add({'id':12,'href':'/','title':"Introduction",'content':"The Event-Driven la1r This site showcases everything which was implemented in my own home la1r. The goal is to see how much (Realtime) Tracking, Automation and AI still brings convenience and what is just flat-out annoying. Since I'm running this at home, me and people in my environment are my test subjects, please regard all the material and integrations tested as such. All the code that's describe on this site can be found at https://github.com/basraven/la1r\nKey topics The following key topics will be touched to showcase la1r:\n Kubernetes, Docker AI, Automation Streaming (Big Data) pipelines Home Automation, Home surveillance And many more things!  Conceptual documentation This documentation captures the underlying concepts, goals and roadmap for the La1r project.\nRead more\n Technical documentation All the details on how each of the concepts is implemented can be found here. It will also contain details on the roadmap based on the conceptual roadmap.\nRead more\n  Planning Working on la1r is a continuous project \u0026amp; learning experience. I created a planning to ensure la1r isn't a fully unguided experiment, but it is focussed on bringing new and complete features. Read more\nWhy this site? I noticed that combining all of these applications into a single integration environment can be challenging to keep all documentation and notes into a single place. To force myself to properly document my steps and also to give back to the open-source community, I decided to publish all of it on this domain. Hopefully it can also help others with similar aspirations. Feel free to share this site with other enthausiasts.\n\u0026ldquo;Please do try this at home\u0026rdquo; Since this overview is focussed on sharing, I would like to invite anyone to try everything at home, it will not have insane hardware requirements or require niche hardware setups.\nI would also invite anyone to do suggestions on github and to file issues when encountered.\nEnjoy! - Seb\n"});})();