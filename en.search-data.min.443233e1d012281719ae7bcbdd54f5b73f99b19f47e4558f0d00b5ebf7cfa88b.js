'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/technical-architecture/','title':"Technical Architecture",'content':"Technical Architecture The technical architecture of La1r is my personal take on how to properly implement the capability architecture as described on this site. This does not mean that all implementations are matching the infrastructure context, on the contrary. Implementations in La1r are aimed on enterprise scale setups, which often make them an overkill for the hardware it is running. The reason for this is that the technical architecture tries to comply to several application architecture principles which are focussed on maximizing my personal learning experiences and are often near enterprise scale.\nMain area\u0026rsquo;s of the technical architecture The technical architecture can be divided into area\u0026rsquo;s:\nApplication Architecture The overview and catalog of all the technical and functional applications running on the platform.\nRead more\n Data Architecture All data processing, migration and storage principles, including AI and Automation.\nRead more\n  Infrastructure Architecture Infrastructure Architecture Design, discussing both Ansible, for initial setup and bare-metal services and Kubernetes, for running all other services.\nRead more\n Security Architecture Setup of several security concepts which are an integral part of the technical architecture.\nRead more\n  Technical Architecture principles The la1r architecture follows several technical principles which components in its architecture should follow. Since this will not capture conceptual principles, a section on conceptual principles is describe in the capability architecture page\n Only the paranoid survive, apply and practice backup scenarios. - Backup scenarios should not only be implemented as tick in the box for our list of non functional requirements (NFRS), but should also be practiced where possible. Aim for near horizontal scaling - all services should be able to scale with cluster size. The infrastructure architecture of my current implementation is rather rigid, but the applications on it should be aimed on flexible and horizontally scalable underlying infrastructure. Behind the VPN (OpenVpn) by default - since this is still a learning and experimental environment, I don\u0026rsquo;t want to think about security first, every step of the way. This is why the master La1r server hosts a VPN virtual network. All services and internal dns are using that entrypoint by default. This does not mean that nothing is exposed to the outside world, but only the services explicitly exposed through the online-traefik instance are  "});index.add({'id':1,'href':'/docs/technical-architecture/application-architecture/','title':"Application Architecture",'content':"Application Architecture There is a large variety of applications currently part of la1r.\nThis page gives an overview of all these applications and also acts as a formal catalog. For that reason, this page will probably change a lot. All applications with date added 14-06-2020 are applications which were added when this page was written and add date was unknown.\nKubernetes Exposed Services    Application Name Status Purpose Namespace (if existing) Url     Kubernetes Dashboard  Management dashboard to manage Kubernetes through a GUI kubernetes-dashboard http://kubernetes.bas   Grafana  Grafana Cluster Dashboards, dashboard for entire cluster monitoring http://grafana.bas   Prometheus  Metrics collector and data store monitoring http://prometheus.bas   Prometheus - Alertmanager  Metrics based alerting monitoring http://alerts.bas   Prometheus - Push Gateway  Push Metrics Endpoint monitoring http://prometheus-push.bas   Prometheus - Blackbox Exporter  Push Blackbox exporter metrics monitoring http://blackbox.bas   Elasticsearch  Elasticsearch data store monitoring http://es.bas   Kibana  Elasticsearch GUI monitoring http://kibana.bas   OpenVPN Status  OpenVPN static metric aggregator on OVPN management port monitoring http://openvpn.bas   Tekton  Cloud-Native CI/CD tekton-pipelines http://tekton.bas   Consul  Consul is used for all internal DNS queries dns http://dns.bas   Traefik 2.x  Reverse Proxy, managing all domain requests on port 33555 kube-system http://treafik.bas   Traefik 2.x - Public  Reverse Proxy for all public endpoints online http://treafik-online.bas   Shields.io endpoint  Endpoint for generating custom shields.io images online http://shields.la1r.com   MotionEye  Security Camera endpoint homeautomation http://cam.bas   Nextcloud  Private storage cloud nextcloud http://cloud.bas   Email - Nextcloud  SMTP Server for sending out emails from Nextcloud nextcloud http://email.bas   Couchpotato  Torrent Movie download library torrent http://couch.bas   Jackett  Torrent Trackers Management torrent http://jackett.bas   Plex  Plex Media Server torrent http://plex.bas   Qbittorent  Bittorrent Download Client torrent http://torrent.bas   Radarr  Movie Torrent data aggregator torrent http://radarr.bas   Sonarr  Series Torrent data aggregator torrent http://sonarr.bas   Bazarr  Subtitle Torrent data aggregator torrent http://bazarr.bas   Zeppelin - Event-Driven  Zeppelin analytics platform for running event-driven analytics analytics https://zeppelin.bas   Nifi - event orchestration  Orchestrating trafic in the analytics architecture analytics https://nifi.bas    Kubernetes Backend Applications    Application Name Status Purpose Namespace (if existing) Folder link Url     Backup jobs  Bi-weekly and monthly backup Kubernetes jobs on all storage Backup Backup N/A   Comms - WhatsApp  REST Endpoint expose Whatsapp client through Selenium Comms WhatsApp http://whatsapp.bas   Comms - Telegram  REST Endpoint exposed Telegram client Comms Telegram http://telegram.bas   FindLF - Wifi Tracking  In-house wifi tracking event FindLF N/A   kafka - event architecture  Kafka cluster which tracks all (home-automation) events event Kafka http://kafka.bas   Zookeeper - Election  Support app for leader election of decentralized infra event zookeeper http://zookeeper.bas   homeassistant  Home Automation Instance, integrates with home automation tools homeautomation Home Assistant http://homeassistant.bas   openhab  Secundairy Home Automation Instance, integrates to homeassistant homeautomation Openhab http://openhab.bas   Weaveworks  L3 Network overlay for Kubernetes kube-system Weaveworks N/A   Weave-scope  Management GUI for Weaveworks network overlay kube-system Weave Scope http://weavescope.bas   Chronograf - Monitoring  Management UI for Chronograf for monitoring monitoring Chronograf Monitoring http://chronograf.bas   Exporter - openvpn  OpenVPN metrics exporter for Prometheus monitoring Exporter Openvpn N/A   Exporter - qbittorent  Qbittorrent metrics exporter for Prometheus monitoring Exporter Qbittorent N/A   Exporter - tautulli  Tautulli metrics exporter for Prometheus monitoring Exporter Tautulli N/A   Exporter - varken  Plex metrics exporter for Prometheus monitoring Exporter Varken N/A   Heimdall  Customizable starting page monitoring Heimdall http://home.bas   InfluxDB - Monitoring  Data Store for Traefik (online \u0026amp; offline) and Varken monitoring InfluxDB Monitoring N/A   MariaDB - Nextcloud  MySQL Database for storing Nextcloud Data nextcloud MariaDB Nextcloud N/A   Redis - Nextcloud  Redis Cache for Nextcloud temp storage nextcloud Redis Nextcloud N/A   Nginx - Nextcloud Public  Public Exposure of some of the Nextcloud resources, filtered online NGINX http://cloud.basraven.nl   Authelia  Authentication Proxy, to secure expose applications security Authelia http://aithelia.bas   Openldap  OpenLDAP IAM implementation, for centralized user management security OpenLDAP http://ldap.bas   Phpladpadmin  OpenLDAP Management UI security PhpLdapAdmin http://openldap.bas     Ansible    Application Name Status Purpose Namespace (if existing) Folder link Url     DnsMasq  Configuration of custom DNS Service from the server Ansible DnsMasq N/A   OVPN  VPN Bare-Metal Service, hosts all major connections Ansible OVPN http://ovpn.bas   Prometheus Node Exporter  Exports all metrics data from each server to Prometheus Ansible Prometheus Node Exporter N/A   Samba  File storage server for local LAN use only, to access data Ansible Samba N/A   NFS  File storage server on external backup storage servers Ansible - Pi NFS N/A    "});index.add({'id':2,'href':'/docs/planning/','title':"Planning",'content':"What\u0026rsquo;s next? Since I\u0026rsquo;m (currently) only developing la1r by myself, there are only so many things you can do at once. (Feel free to reach out through Github if you want to get involved!) For this reason I created this planning page in which I track and prioritize what I will add to la1r next. Feel free to add comments on this through Github!\nIn progress  Node failure alert in case Kubernetes is also not available (keepalived), the VIP can also be used for this  Bugs  Fix end-to-end backups  Requires: Hardware Node DNS names   Fix logging implementation with Elastic Search Network gateway (through Traefik?) for shields.io implementation Fix Kirby implementation  Planned  Home automation reimplementation find-lf - wifi tracking as input events on the Event Bus based on AI model Last events pipeline, store and dashboard implementation Update Network Architecture  Longstay parking  Apache airflow to integrate with streaming pipelines for event-driven home: https://airflow.apache.org/docs/stable/kubernetes.html New doorbell security firmware Fail2ban new filters for ALL services Traefik auth proxy middleware with Authelia \u0026ldquo;View in repo\u0026rdquo; button for all pages of the la1r documentation. While reading documentation, for example about Ansible, the visitor should be able to view which scripts are currently discussed by clicking a button to the git repository. Cronacle cron manager https://github.com/jhuckaby/Cronicle An event data dashboard for kafka Streaming Facial recognition from images and streaming video Object recognition (garbage bin outside of our house) combined with garbage collection iCal (https://inzamelkalender.gad.nl/ical/0402200001574396) Refactor Mosquito to vernemq  Completed  Online reimplementation Video security storage process SNMP of DHCP server to consul/nodes for live node status info (added with a health check / ping performed by consul) Spark 3.x Cluster in k8s Backup reimplementation node_exporter in DaemonSet: https://github.com/prometheus-operator/kube-prometheus/blob/master/manifests/node-exporter-daemonset.yaml Kibana log analysis Monitoring with kube-state-metrics in grafana Monitoring extension, e.g. with alert manager and https://github.com/benjojo/alertmanager-discord Prometheus alert manager implementation and https://awesome-prometheus-alerts.grep.to/rules.html DNS black-hole with pihole Monitoring reimplementation DNS on LAN implementation Traefik 2.x reimplementation Metallb implementation NFS based dynamic storage provisioning New multi-cluster setup with kube-proxy New network configuration with new network hardware Kerberos setup Metallb tests new dns configuration Refactor backup facilities to use compression and dual backup (weekly and bi-daily) OpenLDAP implementation Formal service bus definition and separation of raw data and defined event data in MQTT  "});index.add({'id':3,'href':'/docs/technical-architecture/infrastructure-architecture/','title':"Infrastructure Architecture",'content':"Infrastructure Architecture The infrastructure setup is one of the biggest topics of this project in terms of size and complexity.\n TODO: Infrastructure Arch Diagram\n How its made How all of this infrastructure is deployed can be found in [the deployment architecture].\nRack Architecture    ID Hostname OS Hardware Description Status Hardware Tier labels     1 jay-c CentOS 8 Ryzen 5600x, 0,5TB NVM Available 1 - cluster la1r/nvm: true, la1r/ssd: true, la1r/hdd: true, la1r/priority: high   2 x86bit CentOS 8 Ryzen 5600x, 0,5TB NVM, 2x RAID0 4TB x300 Coming Soon 1 - cluster la1r/nvm: true, la1r/ssd: true, la1r/hdd: true, la1r/priority: high   3 linux-wayne Ubuntu server (latest) Intel Core i5, 0,5TB SSD Available 1 - cluster la1r/shdd: true   4 ali-bel Raspbian Doorbell Raspberry Pi Zero Not Available 2 - periferal    5 kodi-e Raspbian Kodi Raspberry Pi 3b bedroom Available 2 - periferal     Kubernetes (k8s) The majority of the applications are hosted through Kubernetes. K8s can be implemented in a million different ways. The implementation on La1r follows a few principles:\n K8s should run on bare metal only - it should not be dependent on any cloud resource, this includes function-as-service (FaaS) implementations such as AWS Lambda. Vanilla k8s should be used - flavored variants of k8s take some controls away from the sys admin and introduce (sometimes unwanted) abstractions. The latest k8s version should be used - this to incentive application of new k8s capabilities Template engines, package managers and operators such as helm are not used - this again takes away a lot of control from the sys admin which takes away the learning experience, thus the fun. K8s over bare metal - since the use of k8s incentives portability of applications, the default hosting approach should be on k8s not on the bare metal server itself (for example through Ansible). Only things that really make sense or are directly needed by the cluster itself, such as OpenVpn, can be implemented directly on the server with Ansible.  Init with kubeadm Since we\u0026rsquo;re using vanilla k8s on bare metal but we do not have all the time in the world, the decision was made to use kubeadm to initialize the cluster and manage node cluster and upgrades. Several upgrades were already performed with this which went flawless, even with multi cpu (x86 and arm7) architecture environments.\nNode agnostic storage (coming soon) Storage is often a difficult topic with k8s bare metal clusters, since the entire aim of k8s is to be independent and decoupled from infrastructure. This is why the current plan is to migrate all storage used by the cluster to a storage abstraction service, creating node agnostic storage facilities everywhere on the k8s cluster. The currently plan is to achieve this with Ceph through Rook.io, which also is an exception on the non-operator application architecture principle as mentioned earlier\nReverse proxy, with dmz-ed online exposure Traefik is used as reverse proxy for all default traffic, this because it easily integrates with k8s and it provides a fancy dashboard UI, and maybe because I\u0026rsquo;m traefik 2.0 contributor\u0026hellip; Here my personal preference is to not follow the custom resource definition (CRD) hype and just use standard ingress objects, also for possible portability reasons if I ever get sick of traefik.\nNetwork Architecture Since the network architecture of the project is not straight-forward, it is handled with several applications, all making a single cluster. Read more \n"});index.add({'id':4,'href':'/docs/technical-architecture/infrastructure-architecture/deployment-architecture/','title':"Deployment Architecture",'content':"Deployment Architecture Two components are used to setup this architecture:\n Ansible, for bare metal server initialization and bare metal services. Kubernetes, for running processes which can be containerized  The aim is to put as many of the services in Kubernetes, keeping the Ansible footprint as small as possible. Unfortunately there always needs to be an initial setup script, which is handled by the small footprint of Ansible. All other services need to run in Kubernetes, to improve portability and manageability.\nAnsible - Prepare the infrastructure and bare-metal services Since there\u0026rsquo;s always a need for installing packages on the nodes directly and I don\u0026rsquo;t want to just use a bunch of shell scripts all configuration and applications outside of k8s is deployed with Ansible which are directed by makefiles. Makefiles because I don\u0026rsquo;t want to remember all commands that I need to spin up ansible by heart, Ansible because I want to semi-formalize the steps I take. The goal here is to document every step, it does not matter how small, into an Ansible Playbook script. These Ansible scripts can be found on https://github.com/basraven/la1r/ansible\nAnsible Technical Tags The following tags are used in Ansible:\n helper - Helper scripts for the run itself, e.g. to determine OS hostname - Set hostname of server reboot - Reboots the machine update - Update the package managers security - Security related packages and update kerberos_client - Install Kerberos (MIT) client and get keys. Server will install if hosts file contains kerberos: server for this server. kerberos_server - Install Kerberos (MIT) server and create keys. Server will install if hosts file contains kerberos: server for this server. toolbox - Placement of /cicd/ansible/toolbox scripts, used for infrastructure management users - Creation of users dns_server - Install DNS Server docker - Install docker nfs_client - Install nfs-client nfs_server - Install nfs-server openvpn_client - Install openvpn client and place certificate from /credentials openvpn_server - Install openvpn server and creates new CA create_ovpn_user - Create a new certificate for openvpn,  Requires ---extra-vars \u0026quot;openvpn_user=someusername\u0026quot;   delete_ovpn_user - Create a new certificate for openvpn  Requires ---extra-vars \u0026quot;openvpn_user=someusername\u0026quot;   node_exporter - Install prometheus_node_exporter haproxy - Install haproxy kubernetes_server - Install Kubernetes init_kubernetes - Run kubeadm init  Optional ---extra-vars \u0026quot;kubernetes_cidr=10.244.0.0/16\u0026quot;   reset_kubernetes - Reset from changes made by kubeadm init and kubeadm join fetch_kubernetes - Fetch the Kubernetes config file and put it in the local folder storage_kubernetes - Install packages as prep for the storage provider join-kubernetes - Join a kubernetes cluster  Requires ---extra-vars \u0026quot;kubernetes_master=8.8.8.8\u0026quot; Requires /credentials/kubernetes/join-token.yaml\u0026quot;    CI with Jenkins A key component of the architecture is that in essence, everything should be able to run without Jenkins, just with Ansible and Kubernetes. Jenkins is used, just to streamline the process.\nJenkins Pipelines Jenkins contains the following pipelines:\n Deploy Ansible Assets Deploy Kubernetes Assets   Makefiles as operators The initial approach was to use makefiles as operators. But this was not scalable, these files became a mess. This is why Jenkins with Configuration As Code was later introduced.\nContents of former makefiles Since I want to formalize everything into scripts, there needs to be a way to formalize how to call the different playbook with the appropriate arguments. This is why the Git repository contains 2 Makefiles. There has been chosen for Makefiles because the way these files are called is extremely predictable make \u0026lt;your command\u0026gt;:\n Makefile for Ansible - This makefile contains all the Ansible Playbook calls which are made to construct la1r on bare metal Makefile for Kubernetes - This makefile contains all the used Kubernetes calls to setup the Kubernetes nodes. This also contains node setup scripts such as applying taints.  "});index.add({'id':5,'href':'/docs/technical-architecture/infrastructure-architecture/storage-architecture/','title':"Storage Architecture",'content':"Storage Architecture Storage architecture can be a complicated topic when working with (Bare Metal) Kubernetes clusters. There are a lot of War stories of organizations unable to restore their storage solution, e.g. in a complex RAID setup on in a distributed storage solutions such as Ceph, GlusterFS or StorageOS.\nIt took a significant amount of time to weigh the pros and cons of these solutions, but eventually I went for the \u0026ldquo;safest\u0026rdquo; solution, at least a solution which could not produce unrestoreable storage solutions as mentioned before:\n Dynamically provisioned NFS storage in Kubernetes  Storage Tiers A few predefined folder structures are used in this setup, each folder to simulate To simulate the behavior of more complex storage solutions:\n   Backup Volatility Speed     - 1: Backed-up (High Availability)  - 2: Not Backed-up (Normal Availability) - 1: Persistent, retained indefinite  - 2: Volatile, removed after 2 weeks - 1: High speed SSD storage  - 2: Normal speed HDD Storage  - 3: Slower speed HDD storage    Generic Instances    Class Code Implemented server(s) Backup Volatility Speed Hostpath Reclaim     111 1: linux-wayne 1 1 1 /mnt/ssd/ha/\u0026lt;service_name\u0026gt; manual   211 1: linux-wayne 2 1 1 /mnt/ssd/na/\u0026lt;service_name\u0026gt; automatic   221 1: linux-wayne 2 2 1 /mnt/ssd/tmp/\u0026lt;service_name\u0026gt; automatic   112 1: linux-wayne 1 1 2 /mnt/hdd/ha/\u0026lt;service_name\u0026gt; manual   212 (default) 1: linux-wayne 2 1 2 /mnt/hdd/na/\u0026lt;service_name\u0026gt; automatic   222 1: linux-wayne 2 2 2 /mnt/hdd/tmp/\u0026lt;service_name\u0026gt; automatic    Specific Instances TODO: update\n   Class Code Implemented server(s) Backup Volatility Speed Persistent Volume (PV) name Hostpath     211 1: linux-wayne 2 1 1 nextcloud-config /mnt/ssd/ha/nextcloud/config/   212 1: linux-wayne 2 1 2 nextcloud-data /mnt/hdd/ha/nextcloud/data/    "});index.add({'id':6,'href':'/docs/','title':"Docs",'content':""});index.add({'id':7,'href':'/','title':"La1r",'content':"The Event-Driven la1r This blog showcases everything which was implemented in my own home la1r. The goal is to see how much (Realtime) Tracking, Automation and AI still brings convenience and what is just flat-out annoying. Since I\u0026rsquo;m running this at home, me and people in my environment are my test subjects, please regard all the material and integrations tested as such. All the code that\u0026rsquo;s describe on this site can be found at https://github.com/basraven/la1r\nKey topics The following key topics will be touched to showcase la1r:\n Kubernetes, Docker AI, Automation Streaming (Big Data) pipelines Home Automation, Home surveillance And many more things!  Capability Architecture This documentation captures the underlying capabilities, concepts, goals and roadmap for the La1r project from a functional perspective.\nIn need of recursive revision!\nRead more\n Technical Architecture All the details on how each of the concepts is implemented can be found here. It will also contain details on the roadmap based on the conceptual roadmap.\nRead more\n  Planning Working on la1r is a continuous project \u0026amp; learning experience. I created a planning to ensure la1r isn\u0026rsquo;t a fully unguided experiment, but it is focussed on bringing new and complete features. Read more\nWhy this site? I noticed that combining all of these applications into a single integration environment can be challenging to keep all documentation and notes into a single place. To force myself to properly document my steps and also to give back to the open-source community, I decided to publish all of it on this domain. Hopefully it can also help others with similar aspirations. Feel free to share this site with other enthusiasts.\nEnjoy! - Seb\n"});index.add({'id':8,'href':'/docs/technical-architecture/infrastructure-architecture/network-architecture/','title':"Network Architecture",'content':"Network Architecture The network setup is of la1r.com is not a simple one, this due to several reasons:\n Multi-site setup, with servers on different locations, connected either directly or through vpn Multi-tenancy setup, multiple layers of access on the network need to be implemented Multiple vpn endpoints, this to connect the different tenants Multiple servers within a single Kubernetes cluster. Virtual network interfaces with virtual network policies in Kubernetes. Multiple networking hardware devices such as routers, managed switches and IOT devices.  To break down these complexities we can split the network architecture into two parts:\n Technical Layer - (OSI layers 1 - 4) Data Layer - (OSI layers 5 - 7)  Network topology table The following table summarizes all network cidrs and addresses\n   Prefix CIDR IP Target vlan     192.168 1.0/24  Empty, not used, will indicate wrongly configured devices -   192.168 2.0/24  Common devices, laptops, phones, etc. 1   192.168 3.0/24  IOT Devices with dedicated connection to server 1   192.168 4.0/24  Network infrastructure, switches, routers, etc 1   192.168 4.0/24 4.1 Router and DHCP Server 1   192.168 4.0/24 4.2 Central network switch 1   192.168 4.0/24 4.3 Access Point living room 1   192.168 4.0/24 4.4 Access Point office 1   192.168 5.0/20  Servers 1   192.168 5.0/20 5.1 linux-wayne 1   192.168 5.0/20 5.2 50centos 1   192.168 5.0/20 5.3 jay-c 1   192.168 5.0/20 5.100 haproxy VIP entrypoint Kubernetes 1   192.168 6.0/24  Kubernetes MetalLB services 1   192.168 6.0/25  LAN Kubernetes MetalLB services 1   192.168 6.0/25 6.1 LAN Traefik 2.x 1   192.168 6.0/25 6.60 Plex server 1   192.168 6.0/25 6.61 qBittorent 1   192.168 6.0/25 6.71 qBittorent listen 1   192.168 6.0/25 6.62 Radarr 1   192.168 6.0/25 6.63 Sonarr 1   192.168 6.0/25 6.64 Bazarr 1   192.168 6.0/25 6.65 Kubernetes Dashboard 1   192.168 6.0/25 6.66 Grafana 1   192.168 6.0/25 6.77 Log server 1   192.168 6.0/25 6.88 Tekton server 1   192.168 6.0/25 6.90 Consul LAN DNS (DNS UDP) 1   192.168 6.0/25 6.91 Consul LAN DNS (Admin UI backup) 1   192.168 6.0/25 6.99 DNS Blackhole (pihole) 1   192.168 6.128/25  Online Kubernetes MetalLB services 1   192.168 6.128/25 6.128 Online Traefik 2.x 1   10.244 0.0/16  Kubernetes internal cidr kubernetes.local   10.8 2.0/24  Shared VPN access openvpn shared   10.8 2.0/24 2.1 Shared VPN server openvpn shared   10.8 4.0/24  Private VPN access openvpn private   10.8 4.0/24 4.1 Private VPN server openvpn private    Technical Layer  TODO: Update diagram with new hardware\n The Technical Layer is structured as follows:\nNetwork with WeaveWorks Initially the decision was made to use flannel as network provider, since this is a pretty standard choice for many k8s implementations. Unfortunately this gave several networking, performance and upgrading issues over time, especially with our multi cpu architecture environment. After a tool selection process weave works came out best because:\n Substantial performance and stability improvements Capable of complex network segregation which flannel was not able to do Easy to setup, even if several sources on the way point out the opposite Bonus: Weave works supplies a fancy and comprehensive dashboard of your entire network  Data Layer The Data Layer is structured as follows:\n"});})();