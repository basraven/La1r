'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/capability-architecture/','title':"Capability Architecture",'content':"Conceptual details To give a 1-minute overview: this is the conceptual model driving La1r: It shows the conceptual layout of how all components in the Event-Driven architecture are structured. Since we are now only focussing on the conceptual aspects, implementation details such as infrastructure stack are not discussed here. The fundamental architecture for La1r follows a Kappa architectural pattern for processing data. This means that the architecture will handle bulk/batch data the same way as it handles streaming/realtime data.\nThere has been made a distinction between two types of data streams:\n Raw Data Stream - Data that is not \u0026ldquo;Governed\u0026rdquo; and does not apply to the imposed event structures as described on this site. This is often the data sink for commercial-of-the-shelf (COTS) components with which need to be integrated. To save the hassle of writing custom extensions to those components, it is easier to push the data to a \u0026ldquo;raw\u0026rdquo; even stream and transform the raw events into structured events which conform to the even structures. Structured Data Stream - This data stream only contains data which conforms to the defined standard for events  To summarize, we identify several conceptual components:\n Raw Data producer - Any sensor, smart camera, etc. which is hooked up to the raw data stream and produces data from which well formed events can be produced. Raw Data Stream - The data vehicle which stores all incoming data which can be used to create events off, this can be very raw measurement data of data structured in an application specific format Streaming Event Transformations - This can be any application which is connected to the Raw Data Stream and is able to create events conforming to the Event Specifications based on the data coming from the Raw Data Stream (this is non-restrictive and can also come from other places) Event Specifications - All specifications used to structure all events which are published on the Structured Event Stream. These Events specifications will also be published on la1r.com. These event specifications are not a direct part of the actual data flow, but are of a significant enough importance to name it in this diagram. Structured Event Stream - The data stream which stores the structured events, forming the logical epi-center of the event-driven la1r. The majority of the events are sourced by transformed raw data from the Raw Data Stream or results of analyzed raw/structured events Streaming Analytics Processes - This is identical to the Automated Event Transformation, only these processes analyze the data to find significant patterns which can be used by other (decoupled) processes further downstream. Structured Event Consumer - This can be any device which consumes events which are published on the Structured Event Stream and acts on it with a certain behavior, for example a light switching on based on an event. This consumer also entails translating the Structured Event into a format a device is able to operate on.  Conceptual Architecture Principles The la1r architecture followes several conceptual principles which components in its architecture should follow. Since this will not capture implementation specific / technical principles, a section on technical principles is describe in the technical architecture page\n Data is realtime and streaming - Always assume that data, streaming through the la1r infrastructure is in streaming \u0026ldquo;format\u0026rdquo;. Do not unnecessarily store it, or batch it when realtime streaming solutions can also be applied Don\u0026rsquo;t assume information share - since an enterprise environment is conceptually simulated, it should also be simulated that (conceptual) teams are not fully aware of all integrations made by other (conceptual) teams. The implications of this is that there is a need for decoupling and formal information definitions. An example of this is the site you\u0026rsquo;re currently reading, but further efforts should be made such as formal separation of layers, environments and data to appropriately conform to this conceptual requirement. Decentralized application paradigms where possible - To support the horizontal scaling capabilities, an effort should be made to apply decentralized paradigms, which often improve scalability and availability when implemented correctly.  Event Specifications Since there needs to be a way of formally converging to an aligned data setup, a formal event specification setup is made. This event specification will dictate how all events in the structured stream should be shaped. Events not conforming to this standard can be disregarded.\nRead more\n Governance Catalogs Since we are still \u0026ldquo;simulating\u0026rdquo; an enterprise environment, and since my own memory is sub-optimal, appropriate governance catalogs need to be setup to fully capture the IT landscape on several domains.\nRead more\n   "});index.add({'id':1,'href':'/docs/technical-architecture/','title':"Technical Architecture",'content':"How it\u0026rsquo;s made The technical architecture of La1r is my personal take on how to properly implement the capability architecture as described on this site. This does not mean that all implementations are matching the infrastructure context, on the contrary. Implementations in La1r are aimed on enterprise scale setups, which often make them an overkill for the hardware it is running. The reason for this is that the technical architecture tries to comply to several application architecture principles which are focussed on maximizing my personal learning experiences and are often near enterprise scale.\nMain area\u0026rsquo;s of the technical architecture The technical architecture can be divided into area\u0026rsquo;s:\nApplication Architecture The overview and catalog of all the technical and functional applications running on the platform.\nRead more\n Data Architecture All data processing, migration and storage principles, including AI and Automation.\nRead more\n  Infrastructure Architecture Infrastructure Architecture Design, discussing both Ansible, for initial setup and bare-metal services and Kubernetes, for running all other services.\nRead more\n Security Architecture Setup of several security concepts which are an integral part of the technical architecture.\nRead more\n  Technical Architecture principles The la1r architecture follows several technical principles which components in its architecture should follow. Since this will not capture conceptual principles, a section on conceptual principles is describe in the capability architecture page\n Only the paranoid survive, apply and practice backup scenarios. - Backup scenarios should not only be implemented as tick in the box for our list of non functional requirements (NFRS), but should also be practiced where possible. Aim for near horizontal scaling - all services should be able to scale with cluster size. The infrastructure architecture of my current implementation is rather rigid, but the applications on it should be aimed on flexible and horizontally scalable underlying infrastructure. Behind the VPN (OpenVpn) by default - since this is still a learning and experimental environment, I don\u0026rsquo;t want to think about security first, every step of the way. This is why the master La1r server hosts a VPN virtual network. All services and internal dns are using that entrypoint by default. This does not mean that nothing is exposed to the outside world, but only the services explicitly exposed through the online-traefik instance are  "});index.add({'id':2,'href':'/docs/technical-architecture/application-architecture/','title':"Application Architecture",'content':"Application Architecture There is a large variety of applications currently running on the platform.\nThis page gives an overview of all these applications and also acts as a formal catalog. For that reason, this page will probably change a lot. All applications with date added 14-06-2020 are applications which were added when this page was written and add date was unknown.\n   Application Name Status Purpose Namespace (if existing) Folder link Url     Local machine        Control - Docker Jumphost  Jumphost on local dev machine to manage the cluster from Windows Local - Control Control N/A   Bian - Experimental AI        Doorbell image recognition  Image Recognition on video with Python 3.x Brian - Image recognition Image Recognition N/A   Zeppelin - Analytics  Zeppelin analytics platform for running spark jobs Brian - Zeppelin Analytics Zeppelin Experimental http://zeppelin.bas   Ansible        DnsMasq  Configuration of custom DNS Service from the server Ansible DnsMasq N/A   OVPN  VPN Bare-Metal Service, hosts all major connections Ansible OVPN http://ovpn.bas   Prometheus Node Exporter  Exports all metrics data from each server to Prometheus Ansible Prometheus Node Exporter N/A   Samba  File storage server for local LAN use only, to access data Ansible Samba N/A   NFS  File storage server on external backup storage servers Ansible - Pi NFS N/A   Kubernetes        Backup jobs  Bi-weekly and monthly backup Kubernetes jobs on all storage Kubernetes - Backup Backup N/A   Comms - WhatsApp  REST Endpoint expose Whatsapp client through Selenium Kubernetes - Comms WhatsApp http://whatsapp.bas   Comms - Telegram  REST Endpoint exposed Telegram client Kubernetes - Comms Telegram http://telegram.bas   FindLF - Wifi Tracking  In-house wifi tracking Kubernetes - event FindLF N/A   influxdb - kafka sink  InfluxDB instance as data sink for Kafka Kubernetes - event InfluxDB Sink N/A   kafka - event architecture  Kafka cluster which tracks all (home-automation) events Kubernetes - event Kafka http://kafka.bas   Nifi - event orchestration  Orchestrating all trafic in the event-driven architecture Kubernetes - event Nifi http://nifi.bas   Mosquito - \u0026ldquo;raw\u0026rdquo; data sink  Data sink for all event data which integrations with MQTT Kubernetes - event Raw N/A   Zeppelin - Event-Driven  Zeppelin analytics platform for running event-driven analytics Kubernetes - event Zeppelin Event http://zeppelin.bas   Zookeeper - Election  Support app for leader election of decentralized infra Kubernetes - event zookeeper http://zookeeper.bas   homeassistant  Home Automation Instance, integrates with home automation tools Kubernetes - homeautomation Home Assistant http://homeassistant.bas   openhab  Secundairy Home Automation Instance, integrates to homeassistant Kubernetes - homeautomation Openhab http://openhab.bas   Kubernetes Dashboard  Management dashboard to manage Kubernetes through a GUI Kubernetes - kubernetes-dashboard Kubernetes Dashboard http://kubernetes.bas   Traefik 2.x  Reverse Proxy, managing all domain requests on port 33555 Kubernetes - kube-system Traefik 2.x http://treafik.bas   Weaveworks  L3 Network overlay for Kubernetes Kubernetes - kube-system Weaveworks N/A   Weave-scope  Management GUI for Weaveworks network overlay Kubernetes - kube-system Weave Scope http://weavescope.bas   Chronograf - Monitoring  Management UI for Chronograf for monitoring Kubernetes - monitoring Chronograf Monitoring http://chronograf.bas   Exporter - openvpn  OpenVPN metrics exporter for Prometheus Kubernetes - monitoring Exporter Openvpn N/A   Exporter - qbittorent  Qbittorrent metrics exporter for Prometheus Kubernetes - monitoring Exporter Qbittorent N/A   Exporter - tautulli  Tautulli metrics exporter for Prometheus Kubernetes - monitoring Exporter Tautulli N/A   Exporter - varken  Plex metrics exporter for Prometheus Kubernetes - monitoring Exporter Varken N/A   Grafana  Grafana Cluster Dashboards, dashboard for entire cluster Kubernetes - monitoring Grafana http://grafana.bas   Heimdall  Customizable starting page Kubernetes - monitoring Heimdall http://home.bas   InfluxDB - Monitoring  Data Store for Traefik (online \u0026amp; offline) and Varken Kubernetes - monitoring InfluxDB Monitoring N/A   OpenVPN Status  OpenVPN static metric aggregator on OVPN management port Kubernetes - monitoring OpenVPN Status http://openvpn.bas   Prometheus  Metrics collector and data store Kubernetes - monitoring Prometheus http://prometheus.bas   Email - Nextcloud  SMTP Server for sending out emails from Nextcloud Kubernetes - nextcloud Email Nextcloud http://email.bas   MariaDB - Nextcloud  MySQL Database for storing Nextcloud Data Kubernetes - nextcloud MariaDB Nextcloud N/A   Nextcloud  Private storage cloud Kubernetes - nextcloud Nextcloud http://cloud.bas   Redis - Nextcloud  Redis Cache for Nextcloud temp storage Kubernetes - nextcloud Redis Nextcloud N/A   Nginx - Nextcloud Public  Public Exposure of some of the Nextcloud resources, filtered Kubernetes - online NGINX http://cloud.basraven.nl   Traefik 2.x - Public  Reverse Proxy for all public endpoints Kubernetes - online Traefik 2.x Online http://treafik-online.bas   Authelia  Authentication Proxy, to secure expose applications Kubernetes - security Authelia http://aithelia.bas   Openldap  OpenLDAP IAM implementation, for centralized user management Kubernetes - security OpenLDAP http://ldap.bas   Phpladpadmin  OpenLDAP Management UI Kubernetes - security PhpLdapAdmin http://openldap.bas   Couchpotato  Torrent Movie download library Kubernetes - torrent coachpotato http://couch.bas   Jackett  Torrent Trackers Management Kubernetes - torrent Jackett http://jackett.bas   Plex  Plex Media Server Kubernetes - torrent Plex http://plex.bas   Qbittorent  Bittorrent Download Client Kubernetes - torrent Qbittorent http://torrent.bas   Radarr  Movie Torrent data aggregator Kubernetes - torrent Radarr http://radarr.bas   Sonarr  Series Torrent data aggregator Kubernetes - torrent Sonarr http://sonarr.bas    "});index.add({'id':3,'href':'/docs/technical-architecture/data-architecture/','title':"Data Architecture",'content':"Data Architecture There are several setups used in the area of Data Architecture and Data Processing. They all focus on contribution enhanced \u0026ldquo;intelligent\u0026rdquo; decision making in the La1r setup.\n(Streaming) Transformations Since there can be a large difference between how data is received, for example for commercial of the shelf (COTS) applications, and how it should be structured conforming to the described event standard, there is a need for (streaming) Transformations.\nStreaming analytics as default Since La1r is applying a Kappa architecture (see capability architecture for more details on this), it is essential that as many of it\u0026rsquo;s processes occur in a streaming fashion. This also includes all the performed analytics. Streaming analytics brings new considerations, such as messaging ordering and quality of prefix data. Since these concepts are handled out-of-the-box (OOTB) in Spark 2.x, Spark 2.x is considered as the default method of applying streaming analytics.\n(Streaming) Analytics To enhance La1r with intelligent analytics and decision making, streaming analytics is applied to facilitate these needs.\nStreaming transformations A several scenario\u0026rsquo;s in la1r, a (streaming) transformation needs to be made to get the raw streaming data in the structured shape. This is often the case because there are several integrations with commercial-of-the-shelf (COTS) products which do not follow the same structured model.\nNifi and GUI as default Transformations should be easily adjustable to quickly fit the changing data needs. For this reason Nifi is used as the default tool for (streaming) data transformations. This is where Nifi, with its easily adjustable GUI can facilitate for accommodating (with realtime changes). Using a COTS solution such as Nifi also helps to reduce the amount of code that needs to be written for performing (sometimes insignificant) transformations.\n"});index.add({'id':4,'href':'/docs/planning/','title':"Planning",'content':"What\u0026rsquo;s next? Since I\u0026rsquo;m (currently) only developing la1r by myself, there are only so many things you can do at once. (Feel free to reach out through Github if you want to get involved!) For this reason I created this planning page in which I track and prioritize what I will add to la1r next. Feel free to add comments on this through Github!\nIn progress  New network configuration Prototype based configuration management  Planned  Grafana backup script with: https://github.com/ysde/grafana-backup-tool GoCD CI/CD for prototype based config management (2 ways) Kibana log analysis Fail2ban new filters for ALL services Traefik auth proxy middleware with Authelia MetalLB tests \u0026ldquo;View in repo\u0026rdquo; button for all pages of the la1r documentation. While reading documentation, for example about Ansible, the visitor should be able to view which scripts are currently discussed by clicking a button to the git repository. Deploy a Kafka bus as structured event bus instead of mqtt Spark 2.x Cluster in k8s Streaming analytics pipeline with Spark 2.x and Kafka An event data dashboard for kafka Streaming Facial recognition from images and streaming video Object recognition (garbage bin outside of our house) combined with garbage collection iCal (https://inzamelkalender.gad.nl/ical/0402200001574396) Formal managed bare-metal security camera setup Refactor Mosquito to vernemq  Parked  Ceph file system based rook.io or on this guide: https://owncloud.org/news/running-owncloud-in-kubernetes-with-rook-ceph-storage-step-by-step/  Completed  new dns configuration Refactor backup facilities to use compression and dual backup (weekly and bi-daily) OpenLDAP implementation Formal service bus definition and separation of raw data and defined event data in MQTT find-lf - wifi tracking as input events on the Event Bus based on AI models  "});index.add({'id':5,'href':'/docs/capability-architecture/event-specifications/','title':"Event Specifications",'content':"Event Specifications The event specifications are a exhaustive list of specifications to which any event should conform. With specifying this, decoupled and predictable interactions between services can be achieved.\nThese types of event specifications are currently defined:\n Content specifications - Specifications putting requirements on the content of the event Attribute specifications - Specifications putting requirements on the attributes of an event  Content specifications  All content of an event should be structured in JSON format There cannot be duplicate data between content and attribute data All events should contain a Unix epoch timestamp with Amsterdam as timezone, \u0026ldquo;unixts\u0026rdquo; should be formated as %i All events should contain an \u0026ldquo;origin\u0026rdquo; reference (e.g. \u0026ldquo;automated/face-recognition/1\u0026rdquo;) The payload of an event can reference external data with \u0026ldquo;payload\u0026rdquo; or can contain string or blob information:  Direct string, blob or other payload formats such as a JSON object Direct source path in cephFS (spoofed until ceph is implemented) Direct protocol link (e.g. tcp://videostream1.bas) Hyperlink (e.g. https://videostream1.la1r.com)    Content Examples   Example 1, a payload with a link\n{ \u0026#34;unixts\u0026#34; : 1578157000, \u0026#34;origin\u0026#34; : \u0026#34;manual/light-switch/6\u0026#34;, \u0026#34;payload\u0026#34; : \u0026#34;tcp://videostream1.la1r.com\u0026#34; }   Example 2, a payload with a JSON blob\n{ \u0026#34;unixts\u0026#34; : 1577157000, \u0026#34;origin\u0026#34; : \u0026#34;automated/picture-on-login/pc-1\u0026#34;, \u0026#34;payload\u0026#34; : { \u0026#34;online-time-seconds\u0026#34; : 3212, \u0026#34;capture-location\u0026#34; : \u0026#34;/captures/temp/weekly-cleaned/picture-on-login/pc-1/1.jpeg\u0026#34; } }   Attribute specifications  The topic modelling standard should be applied on any event published Attributes need to be as specific as possible, it should not attempt to group multiple entities because this can be achieved by more advanced queries in the event bus protocol by the use of wildcards.  Topic modelling standard Topic modelling is a conceptual architectural decision which should be made very conscious of future extensions. Large refactor movements in the conceptual structure of an Event bus can have a major change impact because many applications need to change their way of interacting.\nWhat needs to be captured in topics The following things arose while brainstorming about what should be captured in a La1r structured event bus:\n This list will probably be extended in the (near) future\n  Any of the flow types  Actuator - such as lights that can dim of switch, these can be binary, stepped or by value. Sensor - such as temperature, presence, location or humidity sensors Intent - this can be behavior, derived from manual input from a person, or based on automated (AI) analytics   Location specificity - is something specific for a location (in the la1r / outside the la1r)? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not location specific. Person or device specificity - is something specific for a person or device? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not person or device specific.  Topic hierarchy Taking this into consideration, all events in the structured event bus need to following this topic hierarchy (all lower case, without spaces or special characters), either indirect by translation from the raw event bus or direct when considering these standards:\n\u0026lt;flow type\u0026gt;/\u0026lt;Location specification\u0026gt;/\u0026lt;Person or device specification\u0026gt; Examples of hierarchy usage   Example 1 - turing off light 1 in the living room\nactuator/living-room/lightswitch-1   Example 2 - security camera 1 sensing an unidentified person\nsensor/front-door/doorbel-camera-1   Example 3 - analysis algo 1 predicts an intent to shutdown all lights in the backyard\nintent/backyard/lightswitch-all   "});index.add({'id':6,'href':'/docs/capability-architecture/governance-catalogs/','title':"Governance Catalogs",'content':"Governance Catalogs Since we are still \u0026ldquo;simulating\u0026rdquo; an enterprise environment, and since my own memory is sub-optimal, appropriate governance catalogs need to be setup to fully capture the IT landscape on several domains:\n Application Catalog - Capture which applications are currently running and are in scope of the la1r environments. Data Catalog - Capture what important data is used where (in the landscape) and again, if it is not in the data catalog it is not regarded as part of the scope of La1r.  The concept of governance catalogs is a reoccurring pattern which large organizations often lack (or at least lack maturity to appropriately apply) in their landscape. A simple principle of \u0026ldquo;if it is not in the domain catalog, it doesn\u0026rsquo;t exist\u0026rdquo; can be regarded in La1r, forcing well-practiced governance.\nWider topic of governance Since governance is a topic which goes far beyond the reach of a catalog, you can regard this page as far from complete. But to ensure I use my time as efficient as possible, I will (now) not dive too deep into other governance practices\n"});index.add({'id':7,'href':'/docs/technical-architecture/infrastructure-architecture/','title':"Infrastructure Architecture",'content':"Infrastructure Architecture The infrastructure setup is one of the biggest topics of this project in terms of size and complexity.\n TODO: Infrastructure Arch Diagram\n How its made How all of this infrastructure is deployed can be found in [the deployment architecture].\nServer Inventory Currently the following servers are being used: | ID | Hostname | OS | Hardware Description | | \u0026mdash; | \u0026mdash; | \u0026mdash; | \u0026mdash; | | 1 | linux-wayne | Ubuntu server (latest) | Core i5 Desktop with SSD | | 2 | 50centos | CentOS 8 | Core i7 Laptop with HDD only |\nKubernetes (k8s) The majority of the applications are hosted through Kubernetes. K8s can be implemented in a million different ways. The implementation on La1r follows a few principles:\n K8s should run on bare metal only - it should not be dependent on any cloud resource, this includes function-as-service (FaaS) implementations such as AWS Lambda. Vanilla k8s should be used - flavored variants of k8s take some controls away from the sys admin and introduce (sometimes unwanted) abstractions. The latest k8s version should be used - this to incentive application of new k8s capabilities Template engines, package managers and operators such as helm are not used - this again takes away a lot of control from the sys admin which takes away the learning experience, thus the fun. K8s over bare metal - since the use of k8s incentives portability of applications, the default hosting approach should be on k8s not on the bare metal server itself (for example through Ansible). Only things that really make sense or are directly needed by the cluster itself, such as OpenVpn, can be implemented directly on the server with Ansible.  Init with kubeadm Since we\u0026rsquo;re using vanilla k8s on bare metal but we do not have all the time in the world, the decision was made to use kubeadm to initialize the cluster and manage node cluster and upgrades. Several upgrades were already performed with this which went flawless, even with multi cpu (x86 and arm7) architecture environments.\nNode agnostic storage (coming soon) Storage is often a difficult topic with k8s bare metal clusters, since the entire aim of k8s is to be independent and decoupled from infrastructure. This is why the current plan is to migrate all storage used by the cluster to a storage abstraction service, creating node agnostic storage facilities everywhere on the k8s cluster. The currently plan is to achieve this with Ceph through Rook.io, which also is an exception on the non-operator application architecture principle as mentioned earlier\nReverse proxy, with dmz-ed online exposure Traefik is used as reverse proxy for all default traffic, this because it easily integrates with k8s and it provides a fancy dashboard UI, and maybe because I\u0026rsquo;m traefik 2.0 contributor\u0026hellip; Here my personal preference is to not follow the custom resource definition (CRD) hype and just use standard ingress objects, also for possible portability reasons if I ever get sick of traefik.\nNetwork Architecture Since the network architecture of the project is not straight-forward, it is handled with several applications, all making a single cluster. Read more \n"});index.add({'id':8,'href':'/docs/technical-architecture/infrastructure-architecture/deployment-architecture/','title':"Deployment Architecture",'content':"Deployment Architecture Two components are used to setup this architecture:\n Ansible, for bare metal server initialization and bare metal services. Kubernetes, for running processes which can be containerized  The aim is to put as many of the services in Kubernetes, keeping the Ansible footprint as small as possible. Unfortunately there always needs to be an initial setup script, which is handled by the small footprint of Ansible. All other services need to run in Kubernetes, to improve portability and manageability.\nAnsible - Prepare the infrastructure and bare-metal services Since there\u0026rsquo;s always a need for installing packages on the nodes directly and I don\u0026rsquo;t want to just use a bunch of shell scripts all configuration and applications outside of k8s is deployed with Ansible which are directed by makefiles. Makefiles because I don\u0026rsquo;t want to remember all commands that I need to spin up ansible by heart, Ansible because I want to semi-formalize the steps I take. The goal here is to document every step, it does not matter how small, into an Ansible Playbook script. These Ansible scripts can be found on https://github.com/basraven/la1r/ansible\nAnsible Tags The following tags are used in Ansible:\n helper - Helper scripts for the run itself, e.g. to determine OS hostname - Set hostname of server reboot - Reboots the machine update - Update the package managers security - Security related packages and update kerberos - Install Kerberos (MIT) client and get keys. Server will install if hosts file contains kerberos: server for this server. toolbox - Placement of /cicd/ansible/toolbox scripts, used for infrastructure management users - Creation of users dns_server - Install DNS Server docker - Install docker nfs_client - Install nfs-client nfs_server - Install nfs-server openvpn_client - Install openvpn client and place certificate from /credentials openvpn_server - Install openvpn server and creates new CA create_ovpn_user - Create a new certificate for openvpn,  Requires ---extra-vars \u0026quot;openvpn_user=someusername\u0026quot;   delete_ovpn_user - Create a new certificate for openvpn  Requires ---extra-vars \u0026quot;openvpn_user=someusername\u0026quot;   node_exporter - Install prometheus_node_exporter kubernetes_server - Install Kubernetes init_kubernetes - Run kubeadm init  Optional ---extra-vars \u0026quot;kubernetes_cidr=10.244.0.0/16\u0026quot;   reset_kubernetes - Reset from changes made by kubeadm init and kubeadm join fetch_kubernetes - Fetch the Kubernetes config file and put it in the local folder storage_kubernetes - Install packages as prep for the storage provider join-kubernetes - Join a kubernetes cluster  Requires ---extra-vars \u0026quot;kubernetes_master=1.1.1.1\u0026quot; Requires /credentials/kubernetes/join-token.yaml\u0026quot;    CI with Jenkins A key component of the architecture is that in essence, everything should be able to run without Jenkins, just with Ansible and Kubernetes. Jenkins is used, just to streamline the process.\nJenkins Pipelines Jenkins contains the following pipelines:\n Deploy Ansible Assets Deploy Kubernetes Assets   Makefiles as operators The initial approach was to use makefiles as operators. But this was not scalable, these files became a mess. This is why Jenkins with Configuration As Code was later introduced.\nContents of former makefiles Since I want to formalize everything into scripts, there needs to be a way to formalize how to call the different playbook with the appropriate arguments. This is why the Git repository contains 2 Makefiles. There has been chosen for Makefiles because the way these files are called is extremely predictable make \u0026lt;your command\u0026gt;:\n Makefile for Ansible - This makefile contains all the Ansible Playbook calls which are made to construct la1r on bare metal Makefile for Kubernetes - This makefile contains all the used Kubernetes calls to setup the Kubernetes nodes. This also contains node setup scripts such as applying taints.  "});index.add({'id':9,'href':'/docs/technical-architecture/infrastructure-architecture/storage-architecture/','title':"Storage Architecture",'content':"Storage Architecture Storage architecture can be a complicated topic when working with (Bare Metal) Kubernetes clusters. There are a lot of War stories of organizations unable to restore their storage solution, e.g. in a complex RAID setup on in a distributed storage solutions such as Ceph, GlusterFS or StorageOS.\nIt took a significant amount of time to weigh the pros and cons of these solutions, but eventually I went for the \u0026ldquo;safest\u0026rdquo; solution, at least a solution which could not produce unrestoreable storage solutions as mentioned before:\n Dynamically provisioned NFS storage in Kubernetes  Storage Tiers A few predefined folder structures are used in this setup, each folder to simulate To simulate the behavior of more complex storage solutions:\n   Backup Volatility Speed     - 1: Backed-up (High Availability)  - 2: Not Backed-up (Normal Availability) - 1: Persistent, retained indefinite  - 2: Volatile, removed after 2 weeks - 1: High speed SSD storage  - 2: Normal speed HDD Storage  - 3: Slower speed HDD storage    Instances:    Class Code Implemented server(s) Hostpath     111 1: linux-wayne /mnt/ssd/ha/\u0026lt;service_name\u0026gt;   211 1: linux-wayne /mnt/ssd/na/\u0026lt;service_name\u0026gt;   221 1: linux-wayne /mnt/ssd/tmp/\u0026lt;service_name\u0026gt;   112 1: linux-wayne /mnt/hdd/ha/\u0026lt;service_name\u0026gt;   212 1: linux-wayne /mnt/hdd/na/\u0026lt;service_name\u0026gt;   222 1: linux-wayne /mnt/hdd/tmp/\u0026lt;service_name\u0026gt;   113 2: 50centos /mnt/slhdd/ha/\u0026lt;service_name\u0026gt;   213 2: 50centos /mnt/slhdd/na/\u0026lt;service_name\u0026gt;   223 2: 50centos /mnt/slhdd/tmp/\u0026lt;service_name\u0026gt;    "});index.add({'id':10,'href':'/docs/technical-architecture/security-architecture/','title':"Security Architecture",'content':"Security Architecture Several security concepts are applied in the implementation, this page highlights a few.\nAnsible Secret Vault Since sensitive data objects such as secrets, certificates and password need to be stored somewhere, and since that location is not my awful memory (I\u0026rsquo;m prone to memory leaks for some reason which are apparently still impossible to solve) I need a way to store this information. An option could be an Ansible secret vault, but since I\u0026rsquo;m not a madman who loves tempting others with putting their encrypted castle keys on git and that doesn\u0026rsquo;t feel future-proof to me, \u0026ldquo;stay quantum save kids!\u0026rdquo; I took a very low level approach:\nOn each git clone on machines I need to use, I also store a /credentials folder which stores all credentials needed for my cluster. I know it\u0026rsquo;s not save either, and I should for sure pgp encrypt that stuff, it still feels more safe than the carrot-stick approach.\nKerberos Kerberos v5 (MIT) is used for securing all NFS Shares. The settings for this is as follows: | Attribute | Value | | \u0026mdash; | \u0026mdash; | | Kerberos server | kerberos.la1r.com | | realm | la1r.com | | admin principal | admin/admin | | client principal |  e.g. 50centos |\n"});index.add({'id':11,'href':'/docs/','title':"Docs",'content':""});index.add({'id':12,'href':'/','title':"La1r",'content':"The Event-Driven la1r This blog showcases everything which was implemented in my own home la1r. The goal is to see how much (Realtime) Tracking, Automation and AI still brings convenience and what is just flat-out annoying. Since I\u0026rsquo;m running this at home, me and people in my environment are my test subjects, please regard all the material and integrations tested as such. All the code that\u0026rsquo;s describe on this site can be found at https://github.com/basraven/la1r\nKey topics The following key topics will be touched to showcase la1r:\n Kubernetes, Docker AI, Automation Streaming (Big Data) pipelines Home Automation, Home surveillance And many more things!  Capability Architecture This documentation captures the underlying capabilities, concepts, goals and roadmap for the La1r project from a functional perspective.\nRead more\n Technical Architecture All the details\\ on how each of the concepts is implemented can be found here. It will also contain details on the roadmap based on the conceptual roadmap.\nRead more\n  Planning Working on la1r is a continuous project \u0026amp; learning experience. I created a planning to ensure la1r isn\u0026rsquo;t a fully unguided experiment, but it is focussed on bringing new and complete features. Read more\nWhy this site? I noticed that combining all of these applications into a single integration environment can be challenging to keep all documentation and notes into a single place. To force myself to properly document my steps and also to give back to the open-source community, I decided to publish all of it on this domain. Hopefully it can also help others with similar aspirations. Feel free to share this site with other enthusiasts.\n\u0026ldquo;Please do try this at home\u0026rdquo; Since this overview is focussed on sharing, I would like to invite anyone to try everything at home, it will not have insane hardware requirements or require niche hardware setups.\nI would also invite anyone to do suggestions on github and to file issues when encountered.\nEnjoy! - Seb\n"});index.add({'id':13,'href':'/docs/technical-architecture/infrastructure-architecture/network-architecture/','title':"Network Architecture",'content':"Network Architecture The network setup is of la1r.com is not a simple one, this due to several reasons:\n Multi-site setup, with servers on different locations, connected either directly or through vpn Multi-tenancy setup, multiple layers of access on the network need to be implemented Multiple vpn endpoints, this to connect the different tenants Multiple servers within a single Kubernetes cluster. Virtual network interfaces with virtual network policies in Kubernetes. Multiple networking hardware devices such as routers, managed switches and IOT devices.  To break down these complexities we can split the network architecture into two parts:\n Technical Layer - (OSI layers 1 - 4) Data Layer - (OSI layers 5 - 7)  Technical Layer The Technical Layer is structured as follows:\nNetwork with WeaveWorks Initially the decision was made to use flannel as network provider, since this is a pretty standard choice for many k8s implementations. Unfortunately this gave several networking, performance and upgrading issues over time, especially with our multi cpu architects environment. After a tool selection process weave works came out best because:\n Substantial performance and stability improvements Capable of complex network segregation which flannel was not able to do Easy to setup, even if several sources on the way point out the opposite Bonus: Weave works supplies a fancy and comprehensive dashboard of your entire network  Data Layer The Data Layer is structured as follows:\n"});})();