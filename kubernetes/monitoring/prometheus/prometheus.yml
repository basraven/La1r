---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |-
    global:
      scrape_interval: 15s
      scrape_timeout: 10s
      evaluation_interval: 15s
    scrape_configs:
    - job_name: 'targetsjson'
      file_sd_configs:
      - files:
        - '/etc/prometheus/targets.json'
    rule_files:
      - /etc/prometheus/rules/*.yml 
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "monitoring-alertmanager-service:9093"

    # - job_name: qbittorent
    #   scrape_interval: 5s
    #   static_configs:
    #   - targets: ['exporter-qbittorent.monitoring:80']
    #     labels:
    #       jobscope: 'application'
    # - job_name: openvpn
    #   scrape_interval: 5s
    #   static_configs:
    #   - targets:
    #     - exporter-openvpn:80
    #     labels:
    #       jobscope: 'application'
    # - job_name: traefik-online
    #   scrape_interval: 3s
    #   static_configs:
    #   - targets:
    #     - online-traefik-ingress-service.online:8082
    #     labels:
    #       jobscope: 'application'

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules-general
  namespace: monitoring
data:
  generalrules.yml: |-
    groups:
    - name: deadmanswitch
      rules:
      - alert: DeadMansSwitch
        annotations:
          description: This is a DeadMansSwitch meant to ensure that the entire Alerting
            pipeline is functional.
          summary: Alerting DeadMansSwitch
        expr: vector(1)
        labels:
          severity: none
    - name: prometheus
      rules:
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus configuration reload failure (instance {{ $labels.instance }})"
          description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: PrometheusAlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus AlertManager configuration reload failure (instance {{ $labels.instance }})"
          description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus not connected to alertmanager (instance {{ $labels.instance }})"
          description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
    - name: node
      rules: 
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host out of memory (instance {{ $labels.instance }})"
          description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      - alert: HostMemoryUnderMemoryPressure
        expr: rate(node_vmstat_pgmajfault[1m]) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host memory under memory pressure (instance {{ $labels.instance }})"
          description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostUnusualNetworkThroughputIn
        expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host unusual network throughput in (instance {{ $labels.instance }})"
          description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostUnusualNetworkThroughputOut
        expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host unusual network throughput out (instance {{ $labels.instance }})"
          description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/rootfs"}  * 100) / node_filesystem_size_bytes{mountpoint="/rootfs"} < 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host out of disk space (instance {{ $labels.instance }})"
          description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostDiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs"}[1h], 4 * 3600) < 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host disk will fill in 4 hours (instance {{ $labels.instance }})"
          description: "Disk will fill in 4 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
      
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host high CPU load (instance {{ $labels.instance }})"
          description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostSystemdServiceCrashed
        expr: node_systemd_unit_state{state="failed"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host SystemD service crashed (instance {{ $labels.instance }})"
          description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"    

      - alert: HostPhysicalComponentTooHot
        expr: node_hwmon_temp_celsius > 75
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Host physical component too hot (instance {{ $labels.instance }})"
          description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: HostNodeOvertemperatureAlarm
        expr: node_hwmon_temp_alarm == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Host node overtemperature alarm (instance {{ $labels.instance }})"
          description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - name: traefik-internal
      rules:
      - alert: TraefikBackendDown
        expr: count(traefik_backend_server_up) by (backend) == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik backend down (instance {{ $labels.instance }})"
          description: "All Traefik backends are down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: TraefikHighHttp4xxErrorRateBackend
        expr: sum(rate(traefik_backend_requests_total{code=~"4.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik high HTTP 4xx error rate backend (instance {{ $labels.instance }})"
          description: "Traefik backend 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: TraefikHighHttp5xxErrorRateBackend
        expr: sum(rate(traefik_backend_requests_total{code=~"5.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik high HTTP 5xx error rate backend (instance {{ $labels.instance }})"
          description: "Traefik backend 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - name: kube-state-metrics
      rules:
      - alert: KubernetesNodeReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
          description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
          description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesStatefulsetDown
        expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
          description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPodNotHealthy
        expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[1h:]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
          description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
          description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesReplicassetMismatch
        expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
          description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
          description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesStatefulsetReplicasMismatch
        expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
          description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - alert: KubernetesDaemonsetMisscheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
          description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"



---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-targets
  namespace: monitoring
data:
  targets.json: |-
    [
      {
        "targets": [
          "localhost:9090"
        ],
        "labels": {
          "targetname": "prometheus-local",
          "jobscope": "application"
        }
      },
      {
        "targets": [
          "192.168.5.1:9100"
        ],
        "labels": {
          "targetname": "linux-wayne",
          "jobscope": "node",
          "nodetype": "cluster-node"
        }
      },
      {
        "targets": [
          "192.168.5.2:9100"
        ],
        "labels": {
          "targetname": "50centos",
          "jobscope": "node",
          "nodetype": "cluster-node"
        }
      },
      {
        "targets": [
          "192.168.9.71:9100"
        ],
        "labels": {
          "targetname": "ali-bel",
          "jobscope": "node",
          "nodetype": "peripheral-node"
        }
      },
      {
        "targets": [
          "192.168.15.243:9100"
        ],
        "labels": {
          "targetname": "kodi-e",
          "jobscope": "node",
          "nodetype": "peripheral-node"
        }
      },
      {
        "targets": [
          "traefik-ingress-service.traefik:8082"
        ],
        "labels": {
          "targetname": "traefik",
          "jobscope": "application"
        }
      },
      {
        "targets": [
          "kube-state-metrics.kube-system:8080"
        ],
        "labels": {
          "targetname": "kube-state-metrics",
          "jobscope": "application"
        }
      }
    ]


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: monitoring-prometheus
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: monitoring-prometheus
    spec:
      containers:
      - name: prometheus
        ports:
        - containerPort: 9090
        image: prom/prometheus
        volumeMounts:
        - name: prometheus-volume
          mountPath: /prometheus
        - name: prometheus-config
          mountPath: /etc/prometheus/prometheus.yml
          subPath: prometheus.yml
        - name: prometheus-targets
          mountPath: /etc/prometheus/targets.json
          subPath: targets.json
        - name: rules-general
          mountPath: /etc/prometheus/rules/
      volumes:
      - name: prometheus-volume
        persistentVolumeClaim:
          claimName: prometheus-claim
      - name: prometheus-config
        configMap:
          name: prometheus-config
          items:
          - key: prometheus.yml
            path: prometheus.yml
          defaultMode: 0744
      - name: prometheus-targets
        configMap:
          name: prometheus-targets
          items:
          - key: targets.json
            path: targets.json
          defaultMode: 0744
      - name: rules-general
        configMap:
          name: prometheus-rules-general
---
apiVersion: v1
kind: Service
metadata:
  labels:
    task: monitoring
  name: monitoring-prometheus
  namespace: monitoring
spec:
  ports:
  - port: 80
    targetPort: 9090
  selector:
    k8s-app: monitoring-prometheus
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: monitoring
  # annotations:
  #   kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: prometheus.bas
    http:
      paths:
      - path: /
        backend:
          serviceName: monitoring-prometheus
          servicePort: 80