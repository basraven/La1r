'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/conceptual-setup/','title':"Conceptual Setup",'content':"Conceptual details To give a 1-minute overview: this is the conceptual model driving La1r: It clearly shows how the Event-Driven architecture is the heart of La1r\nTo summarize, we identify several conceptual components:\n Raw Data producer - Any sensor, smart camera, etc. which is hooked up to the raw data bus and produces data from which well formed events can be produced. Raw Data Bus - The data vehicle which stores all incoming data which can be used to create events off, this can be very raw measurement data of data structured in an application specific format Automation Event Transformation - This can be any application which is connected to the Raw Data Bus and is able to create events conforming to the Event Specifications based on the data coming from the Raw Data Bus (this is non-restrictive and can also come from other places) Event Specifications - All specifications used to structure all events which are published on the Structured Event Bus. These Events specifications will also be published on la1r.com Structured Event Bus - The data vehicle which stores the structured events, forming the logical epi-center of the event-driven la1r. The majority of the events are sourced by transformed raw data from the Raw Data Bus AI Processes - This is identical to the Automated Event Transformation, only these processes primarily involve AI to add events to the Structured Events Bus. In addition it can take already published events and create new events on that behavior based on predictive models. Structured Event Consumer - This can be any device which consumes events which are published on the Structured Event Bus and acts on it with a certain behavior, for example a light switching on based on an event. This consumer also entails translating the Structured Event into a format a device is able to operate on.  "});index.add({'id':1,'href':'/docs/technical-setup/','title':"Technical Setup",'content':"How it's made The technical setup of La1r is my personal take on how to properly implement the conceptual setup as described on this site. This does not mean that all implementations are matching the infrastructure context, on the contrary. Implementations in La1r are aimed on enterprise scale setups, which often make them an overkill for the hardware it is running. The reason for this is that the technical setup tries to comply to several application architecture principles which are focussed on maximizing my personal learning experiences and are often near enterprise scale.\nMain area's of the technical setup The technical setup can be divided into area's:\nAI, Automation, Analytics All the components which have a primary nature in AI, Automation and Analytics\nRead more\n Ansible All bare-metal setup related Ansible scripts. This is mainly used to setup new hardware and to manage hardware such as IOT devices or new servers (improving portability).\nRead more\n Kubernetes All Kubernetes related scripts, the core of La1r! All components will be documented.\nRead more\n Secrets Small document describing how secrets are managed in the different Ansible and k8s clusters.\nRead more\n  Application architecture principles  Only the paranoid survive, apply and practice backup scenarios. - Backup scenarios should not only be implemented as tick in the box for our list of non functional requirements (nfrs), but should also be practiced where possible. Aim for near horizontal scaling - all services should be able to scale with cluster size. The infrastructure architecture of my current implementation is rather rigid, but the applications on it should be aimed on flexible and horizontally scalable underlying infrastructure. Decentralized application paradigms where possible - To support the horizontal scaling capabilities, an effort should be made to apply decentralized paradigms, which often improve scalability and availability when implemented correctly. Don't assume information share - since an enterprise environment is conceptually simulated, it should also be simulated that (conceptual) teams are not fully aware of all integrations made by other (conceptual) teams. The implications of this is that there is a need for decoupling and formal informations definitions. An example of this is the site you're currently reading, but further efforts should be made such as formal separation of layers, environments and data to appropriately conform to this conceptual requirement. Behind the VPN (openvpn) by default - since this is still a learning and experimental environment, I don't want to think about security first, every step of the way. This is why the master La1r server hosts a VPN virtual network. All services and internal dns are using that entrypoint by default. This does not mean that nothing is exposed to the outside world, but only the services explicitly exposed through the online-traefik instance are  "});index.add({'id':2,'href':'/docs/technical-setup/analytics/','title':"AI, Automation, Analytics",'content':"AI, Automation and Analytics There are several setups used in the area of AI, Automation and Analytics.\nActive AI applications All the components which have a primary nature in AI which are currently used\nRead more\n Analytics All analytics based instances\nRead more\n Automation All automation instances based on analytics / AI\nRead more\n  "});index.add({'id':3,'href':'/docs/planning/','title':"Planning",'content':"What's next? Since I'm (currently) only developing la1r by myself, there are only so many things you can do at once. For this reason I created this planning page in which I track and prioritize what I will add to la1r next. Feel free to add comments on this through Github!\nIn progress Planned  OpenLDAP implementation Formal managed bare-metal security setup Keycloak proxy \u0026amp; management plane Ceph file system based rook.io or on this guide: https://owncloud.org/news/running-owncloud-in-kubernetes-with-rook-ceph-storage-step-by-step/ Refactor mosquitto to vernemq  Parked  Deploy a Kafka bus as structured event bus instead of mqtt  Parked due to server resources shortage   An event data dashboard for kafka  Completed  Formal service bus definition and separation of raw data and defined event data in MQTT find-lf - wifi tracking as input events on the Event Bus based on AI models  "});index.add({'id':4,'href':'/docs/conceptual-setup/event-specifications/','title':"Event Specifications",'content':"Event Specifications The event specifications are a exhaustive list of specifications to which any event should conform. With specifying this, decoupled and predictable interactions between services can be achieved.\nThese types of event specifications are currently defined:\n Content specifications - Specifications putting requirements on the content of the event Attribute specifications - Specifications putting requirements on the attributes of an event  Content specifications  All content of an event should be structured in JSON format There cannot be duplicate data between content and attribute data All events should contain a Unix epoch timestamp with Amsterdam as timezone, \u0026ldquo;unixts\u0026rdquo; should be foramted as %i All events should contain an \u0026ldquo;origin\u0026rdquo; reference (e.g. \u0026ldquo;automated/face-recognition/1\u0026rdquo;) The payload of an event can reference external data with \u0026ldquo;payload\u0026rdquo; or can contain string or blob information:  Direct string, blob or other payload formats such as a JSON object Direct source path in cephFS (spoofed until ceph is implemented) Direct protocol link (e.g. tcp://videostream1.bas) Hyperlink (e.g. https://videostream1.la1r.com)    Content Examples  Example 1, a payload with a link { \u0026#34;unixts\u0026#34; : 1578157000, \u0026#34;origin\u0026#34; : \u0026#34;manual/light-switch/6\u0026#34;, \u0026#34;payload\u0026#34; : \u0026#34;tcp://videostream1.la1r.com\u0026#34; }  Example 2, a payload with a JSON blob { \u0026#34;unixts\u0026#34; : 1577157000, \u0026#34;origin\u0026#34; : \u0026#34;automated/picture-on-login/pc-1\u0026#34;, \u0026#34;payload\u0026#34; : { \u0026#34;online-time-seconds\u0026#34; : 3212, \u0026#34;capture-location\u0026#34; : \u0026#34;/captures/temp/weekly-cleaned/picture-on-login/pc-1/1.jpeg\u0026#34; } }   Attribute specifications  The topic modelling standard should be applied on any event published Attributes need to be as specific as possible, it should not attempt to group multiple entities because this can be achieved by more advanced queries in the event bus protocol by the use of wildcards.  Topic modelling standard Topic modelling is a conceptual architectural decision which should be made very conscious of future extensions. Large refactor movements in the conceptual structure of an Event bus can have a major change impact because many applications need to change their way of interacting.\nWhat needs to be captured in topics The following things arose while brainstorming about what should be captured in a La1r structured event bus:\n This list will probably be extended in the (near) future\n  Any of the flow types  Actuator - such as lights that can dim of switch, these can be binary, stepped or by value. Sensor - such as temperature, presence, location or humidity sensors Intent - this can be behavior, derived from manual input from a person, or based on automated (AI) analytics   Location specificity - is something specific for a location (in the la1r / outside the la1r)? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not location specific. Person or device specificity - is something specific for a person or device? Numbering with \u0026ldquo;-%i\u0026rdquo; as template. \u0026ldquo;-\u0026rdquo; if not person or device specific.  Topic hierarchy Taking this into consideration, all events in the structured event bus need to following this topic hierarchy (all lower case, without spaces or special characters), either indirect by translation from the raw event bus or direct when considering these standards:\n\u0026lt;flow type\u0026gt;/\u0026lt;Location specification\u0026gt;/\u0026lt;Person or device specification\u0026gt; Examples of hierarchy usage  Example 1 - turing off light 1 in the living room actuator/living-room/lightswitch-1  Example 2 - security camera 1 sensing an unidentified person sensor/front-door/doorbel-camera-1  Example 3 - analysis algo 1 predicts an intent to shutdown all lights in the backyard intent/backyard/lightswitch-all   "});index.add({'id':5,'href':'/docs/conceptual-setup/structured-event-bus/','title':"Structured Event Bus",'content':"Structured Event Bus The data vehicle which stores the structured events, forming the logical epi-center of the event-driven la1r. The majority of the events are sourced by transformed raw data from the Raw Data Bus\n All events in the structured event bus conform to the [./event-specifications] to be formally \u0026ldquo;accepted\u0026rdquo; The current goal is to have a single event bus through which the majority (if it makes sense) of the applications communicate through. All analytics should be done on the Structured event bus, never on the raw data bus  "});index.add({'id':6,'href':'/docs/technical-setup/analytics/Analytics/','title':"Analytics",'content':"Coming soon "});index.add({'id':7,'href':'/docs/technical-setup/analytics/Automation/','title':"Automation",'content':"Coming soon "});index.add({'id':8,'href':'/docs/technical-setup/ansible/','title':"Ansible",'content':"Ansible to prepare the playground Since there's always a need for installing packages on the nodes directly and I don't want to just use a bunch of shell scripts all configuration and applications outside of k8s is deployed with Ansible which are directed by makefiles. Makefiles because I don't want to remember all commands that I need to spin up ansible by heart,Ansible because I want to semi-formalize the steps I take.\n"});index.add({'id':9,'href':'/docs/technical-setup/kubernetes/','title':"Kubernetes",'content':"How we do Kubernetes (k8s) K8s can be implemented in a million different ways. The implementation on La1r follows a few principles:\n K8s should run on bare metal only - it should not be dependent on any cloud resource, this includes function-as-service (FaaS) implementations such as AWS Lambda. Vanilla k8s should be used - flavored varients of k8s take some controls away from the sys admin and introduce (sometimes unwanted) abstractions. The latest k8s version should be used - this to incentive application of new k8s capabilities Templating engines, package managers and operators such as helm are not used - this again takes away a lot of control from the sys admin which takes away the learning experience, thus the fun. K8s over bare metal - since the use of k8s incentives portability of applications, the default hosting approach should be on k8s not on the bare metal server itself (for example through Ansible). Only things that really make sense or are directly needed by the cluster itself, such as openvpn, can be implemented directly on the server with Ansible.  Init with kubeadm Since we're using vanilla k8s on bare metal but we do not have all the time in the world, the decision was made to use kubeadm to initialize the cluster and manage node cluster and upgrades. Several upgrades were already performed with this which went flawless, even with multi cpu (x86 and arm7) architecture environments.\nNetwork with weaveworks Initially the decision was made to use flannel as network provider, since this is a pretty standard choice for many k8s implementations. Unfortunately this gave several networking, performance and upgrading issues over time, especially with our multi cpu architecte environment. After a tool selection process weave works came out best because:\n Substantial performance and stability improvements Capable of complex network segregations which flannel was not able to do Easy to setup, even if several sources on the way point out the opposite Bonus: Weave works supplies a fancy and comprehensive dashboard of your entire network  Node agnostic storage (coming soon) Storage is often a difficult topic with k8s bare metal clusters, since the entire aim of k8s is to be independent and decoupled from infrastructure. This is why the current plan is to migrate all storage used by the cluster to a storage abstraction service, creating node agnostic storage facilities everywhere on the k8s cluster. The currently plan is to achieve this with ceph through rook.io, which also is an exception on the non-operator application architecture principle as mentioned earlier\nReverse proxy, with dmz-ed online exposure Traefik is used as reverse proxy for all default traffic, this because it easily integrates with k8s and it provides a fancy dashboard UI, and maybe because I'm traefik 2.0 contributer\u0026hellip; Here my personal preference is to not follow the custom resource definition (CRD) hype and just use standard ingress objects, also for possible portability reasons if I ever get sick of traefik.\n"});index.add({'id':10,'href':'/docs/technical-setup/secrets/','title':"Secrets",'content':"Secrets, certificates and passwords Since sensitive data objects such as secrets, certificates and password need to be stored somewhere, and since that location is not my awful memory (I'm prone to memory leaks for some reason which are apparently still impossible to solve) I need a way to store this information. An option could be an Ansible secret vault, but since I'm not a madman who loves tempting others with putting their encrypted castle keys on git and that doesn't feel future-proof to me, \u0026ldquo;stay quantum save kids!\u0026rdquo; I took a very low level approach:\nOn each git clone on machines I need to use, I also store a /credentials folder which stores all credentials needed for my cluster. I know it's not save either, and I should for sure pgp encrypt that stuff, it still feels more safe than the carrot-stick approach\n"});index.add({'id':11,'href':'/docs/technical-setup/analytics/active-ai-applications/','title':"Active AI applications",'content':"Coming soon "});index.add({'id':12,'href':'/docs/conceptual-setup/analytics-concepts/','title':"Analyitics Concepts",'content':"Analytics Concepts Analytics is a very broad area in la1r. There are a variaty of implementation for this, from indoor location tracking to facial recognition. All these forms of analyitics adhere to a few central principles in the way they are set up.\nAnalytics principles  Realtime streaming analytics by default - If it is possible to apply analytics in realtime, an effort should be made to  "});index.add({'id':13,'href':'/docs/','title':"Docs",'content':""});index.add({'id':14,'href':'/','title':"Introduction",'content':"The Event-Driven la1r This site showcases everything which was implemented in my own home la1r. The goal is to see how much Tracking, Automation and AI still brings convenience and what is just flat-out annoying. Since I'm running this at home, me and people in my environment are my test subjects, please regard all the material and integrations tested as such.\nKey topics The following key topics will be touched to showcase la1r:\n Kubernetes, Docker AI, Automation Home Automation, Home surveillance And many more things!  Conceptual documentation This documentation captures the underlying concepts, goals and roadmap for the La1r project.\nRead more\n Technical documentation All the details on how each of the concepts is implemented can be found here. It will also contain details on the roadmap based on the conceptual roadmap.\nRead more\n  Planning Working on la1r is a continuous project \u0026amp; learning experience. I created a planning to ensure la1r isn't a fully unguided experiment, but it is focussed on bringing new and complete features. Read more\nWhy this site? I noticed that combining all of these applications into a single integration environment can be challenging to keep all documentation and notes into a single place. To force myself to properly document my steps and also to give back to the open-source community, I decided to publish all of it on this domain. Hopefully it can also help others with similar aspirations.\n\u0026ldquo;Please do try this at home\u0026rdquo; Since this overview is focussed on sharing, I would like to invite anyone to try everything at home, it will not have insane hardware requirements or require niche hardware setups.\nI would also invite anyone to do suggestions on github and to file issues when encountered.\nEnjoy! - Seb\n"});})();